"""
OpenAI Provider - å¸¦è¶…æ—¶å’Œé‡è¯•æœºåˆ¶
"""

from __future__ import annotations
import os
import asyncio
from typing import AsyncIterator

from .base import BaseLLMProvider, StreamChunk, LLMConfig, DEFAULT_LLM_CONFIG


class OpenAIProvider(BaseLLMProvider):
    """OpenAI Provider (ä¹Ÿå…¼å®¹ OpenAI å…¼å®¹çš„ API)"""

    def __init__(
        self,
        api_key: str = None,
        base_url: str = None,
        model: str = "gpt-4o",
        config: LLMConfig = None
    ):
        from openai import AsyncOpenAI

        self.model = model
        self.config = config or DEFAULT_LLM_CONFIG
        self.client = AsyncOpenAI(
            api_key=api_key or os.getenv("OPENAI_API_KEY"),
            base_url=base_url or os.getenv("OPENAI_BASE_URL"),
            timeout=self.config.timeout,
            max_retries=0  # æˆ‘ä»¬è‡ªå·±å¤„ç†é‡è¯•
        )

    async def stream(
        self,
        messages: list[dict],
        tools: list[dict] = None,
        temperature: float = 0.7,
        max_tokens: int = 4096,
        timeout: float = None,
    ) -> AsyncIterator[StreamChunk]:
        kwargs = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": True,
        }
        if tools:
            kwargs["tools"] = tools
            kwargs["tool_choice"] = "auto"

        timeout = timeout or self.config.timeout
        last_error = None

        # é‡è¯•å¾ªçŽ¯
        for attempt in range(self.config.max_retries + 1):
            try:
                # å¸¦è¶…æ—¶çš„ API è°ƒç”¨
                stream = await asyncio.wait_for(
                    self.client.chat.completions.create(**kwargs),
                    timeout=self.config.connect_timeout
                )

                current_tool_calls = {}
                last_chunk_time = asyncio.get_event_loop().time()

                async for chunk in stream:
                    # æ£€æŸ¥æµå¼å“åº”æ˜¯å¦è¶…æ—¶ï¼ˆé•¿æ—¶é—´æ²¡æœ‰æ–°æ•°æ®ï¼‰
                    current_time = asyncio.get_event_loop().time()
                    if current_time - last_chunk_time > 60:  # 60ç§’æ²¡æœ‰æ•°æ®
                        yield StreamChunk(
                            type="error",
                            error="æµå¼å“åº”è¶…æ—¶ - 60ç§’æœªæ”¶åˆ°æ•°æ®"
                        )
                        return
                    last_chunk_time = current_time

                    delta = chunk.choices[0].delta if chunk.choices else None

                    if delta is None:
                        continue

                    # æ–‡æœ¬å†…å®¹
                    if delta.content:
                        yield StreamChunk(type="text", content=delta.content)

                    # å·¥å…·è°ƒç”¨
                    if delta.tool_calls:
                        for tc in delta.tool_calls:
                            tc_id = tc.id or list(current_tool_calls.keys())[-1] if current_tool_calls else None

                            if tc.id:  # æ–°çš„å·¥å…·è°ƒç”¨
                                current_tool_calls[tc.id] = {
                                    "id": tc.id,
                                    "name": tc.function.name if tc.function else "",
                                    "arguments": ""
                                }
                                yield StreamChunk(
                                    type="tool_call",
                                    tool_call_id=tc.id,
                                    tool_name=tc.function.name if tc.function else None
                                )

                            if tc.function and tc.function.arguments:
                                if tc_id and tc_id in current_tool_calls:
                                    current_tool_calls[tc_id]["arguments"] += tc.function.arguments
                                yield StreamChunk(
                                    type="tool_call_delta",
                                    tool_call_id=tc_id,
                                    tool_args_delta=tc.function.arguments
                                )

                    # å®Œæˆ
                    if chunk.choices[0].finish_reason:
                        # è§£æžå®Œæ•´çš„å·¥å…·è°ƒç”¨å‚æ•°
                        for tc_id, tc_data in current_tool_calls.items():
                            try:
                                import json
                                tc_data["arguments"] = json.loads(tc_data["arguments"])
                            except:
                                pass

                        # ç»Ÿä¸€ usage å­—æ®µï¼Œç¡®ä¿åŒ…å« input_tokens/output_tokens/total_tokens
                        usage = None
                        if chunk.usage:
                            raw = chunk.usage.model_dump()
                            usage = {
                                "input_tokens": raw.get("prompt_tokens", 0),
                                "output_tokens": raw.get("completion_tokens", 0),
                                "total_tokens": raw.get("total_tokens", 0),
                            }
                        yield StreamChunk(
                            type="finish",
                            finish_reason=chunk.choices[0].finish_reason,
                            usage=usage
                        )
                        
                        # æˆåŠŸå®Œæˆï¼Œé€€å‡ºé‡è¯•å¾ªçŽ¯
                        return

            except asyncio.TimeoutError:
                last_error = f"è¿žæŽ¥è¶…æ—¶ (>{self.config.connect_timeout}s)"
                if attempt < self.config.max_retries:
                    delay = self.config.retry_delay * (2 ** attempt)
                    yield StreamChunk(
                        type="error",
                        error=f"â±ï¸ {last_error} - ç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {delay:.1f}s..."
                    )
                    await asyncio.sleep(delay)
                    continue

            except asyncio.CancelledError:
                raise  # ç”¨æˆ·ä¸­æ–­ï¼Œç›´æŽ¥æŠ›å‡º

            except Exception as e:
                last_error = str(e)
                error_lower = last_error.lower()

                # åˆ¤æ–­æ˜¯å¦å¯é‡è¯•
                retryable = any(keyword in error_lower for keyword in [
                    "timeout", "connection", "network", "rate", "429",
                    "500", "502", "503", "504", "overloaded"
                ])

                if retryable and attempt < self.config.max_retries:
                    delay = self.config.retry_delay * (2 ** attempt)

                    # å¦‚æžœæ˜¯é€ŸçŽ‡é™åˆ¶ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
                    if "rate" in error_lower or "429" in last_error:
                        delay = max(delay, 10.0)

                    yield StreamChunk(
                        type="error",
                        error=f"ðŸ”„ {type(e).__name__}: {last_error[:100]} - ç¬¬ {attempt + 1} æ¬¡é‡è¯•ï¼Œç­‰å¾… {delay:.1f}s..."
                    )
                    await asyncio.sleep(delay)
                    continue
                else:
                    # ä¸å¯é‡è¯•æˆ–å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°
                    yield StreamChunk(
                        type="error",
                        error=f"âŒ {type(e).__name__}: {last_error}"
                    )
                    yield StreamChunk(
                        type="finish",
                        finish_reason="error"
                    )
                    return

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        yield StreamChunk(
            type="error",
            error=f"âŒ å·²é‡è¯• {self.config.max_retries} æ¬¡ä»ç„¶å¤±è´¥: {last_error}"
        )
        yield StreamChunk(
            type="finish",
            finish_reason="error"
        )
